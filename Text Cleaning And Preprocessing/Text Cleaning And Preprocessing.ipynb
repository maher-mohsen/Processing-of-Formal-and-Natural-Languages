{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assi #1 NLP&FLP\n",
        "## Silvana Yacoub 20201091\n",
        "## Maher Mohsen 20200415\n",
        "## S1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wlc7KM-dXKYA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\mezwm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\mezwm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\mezwm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YPZ5MCo7XOUc"
      },
      "outputs": [],
      "source": [
        "# Step 1: Fetch HTML content from a URL\n",
        "def get_html_text(url):\n",
        "    response = requests.get(url)\n",
        "    html_text = response.text\n",
        "    return html_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hvXB7ETCXWMm"
      },
      "outputs": [],
      "source": [
        "def extract_text(html_text):\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    # Extract text from paragraphs and headings\n",
        "    text = ' '.join([p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wCvnyKr_YdnY"
      },
      "outputs": [],
      "source": [
        "# Step 3: Cleaning data\n",
        "def clean_data(text):\n",
        "    # Remove non-alphanumeric characters and symbols\n",
        "    \n",
        "    replaced_text = text.replace('[edit]', '')\n",
        "    replaced_text = text.replace('-', ' ')\n",
        "    replaced_text = text.replace('/', ' ')\n",
        "    \n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', replaced_text)\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SguNrdvaYiJd"
      },
      "outputs": [],
      "source": [
        "# Step 4: Normalization\n",
        "def normalize_text(text):\n",
        "    normalized_text = text.lower()\n",
        "    return normalized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p7HlOFRVYmFY"
      },
      "outputs": [],
      "source": [
        "# Step 5: Tokenization\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LkM2OQgOYoRo"
      },
      "outputs": [],
      "source": [
        "# Step 6: Lemmatization\n",
        "def lemmatize_text(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EGGXT3D8YwHm"
      },
      "outputs": [],
      "source": [
        "# Step 7: Remove stop words\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sD-ulZfrYzDQ"
      },
      "outputs": [],
      "source": [
        "# Step 8: Get unique words\n",
        "def get_unique_words(tokens):\n",
        "    unique_words = set(tokens)\n",
        "    return unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfY18tksY28v",
        "outputId": "f3e6abe6-855c-4f1e-8eb6-715be94037c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents Neural architecture search Neural architecture search (NAS)[1][2] is a technique for automating the design of artificial neural networks (ANN), a widely used model in the field of machine learning. NAS has been used to design networks that are on par or outperform hand-designed architectures.[3][4] Methods for NAS can be categorized according to the search space, search strategy and performance estimation strategy used:[1]\n",
            " NAS is closely related to hyperparameter optimization[5] and meta-learning[6] and is a subfield of automated machine learning (AutoML).\n",
            " Reinforcement learning[edit] Reinforcement learning (RL) can underpin a NAS search strategy. Barret Zoph and Quoc Viet Le[3] applied NAS with RL targeting the CIFAR-10 dataset and achieved a network architecture that rivals the best manually-designed architecture for accuracy, with an error rate of 3.65, 0.09 percent better and 1.05x faster than a related hand-designed model. On the Penn Treebank dataset, that model composed a recurrent cell that outperforms LSTM, reaching a test set perplexity of 62.4, or 3.6 perplexity better than the prior leading system. On the PTB character language modeling task it achieved bits per character of 1.214.[3]\n",
            " Learning a model architecture directly on a large dataset can be a lengthy process. NASNet[4][7] addressed this issue by transferring a building block designed for a small dataset to a larger dataset. The design was constrained to use two types of convolutional cells to return feature maps that serve two main functions when convoluting an input feature map: normal cells that return maps of the same extent (height and width) and reduction cells in which the returned feature map height and width is reduced by a factor of two. For the reduction cell, the initial operation applied to the cell's inputs uses a stride of two (to reduce the height and width).[4] The learned aspect of the design included elements such as which lower layer(s) each higher layer took as input, the transformations applied at that layer and to merge multiple outputs at each layer. In the studied example, the best convolutional layer (or \"cell\") was designed for the CIFAR-10 dataset and then applied to the ImageNet dataset by stacking copies of this cell, each with its own parameters. The approach yielded accuracy of 82.7% top-1 and 96.2% top-5. This exceeded the best human-invented architectures at a cost of 9 billion fewer FLOPSâ€”a reduction of 28%. The system continued to exceed the manually-designed alternative at varying computation levels. The image features learned from image classification can be transferred to other computer vision problems. E.g., for object detection, the learned cells integrated with the Faster-RCNN framework improved performance by 4.0% on the COCO dataset.[4]\n",
            " In the so-called Efficient Neural Architecture Search (ENAS), a controller discovers architectures by learning to search for an optimal subgraph within a large graph. The controller is trained with policy gradient to select a subgraph that maximizes the validation set's expected reward. The model corresponding to the subgraph is trained to minimize a canonical cross entropy loss. Multiple child models share parameters, ENAS requires fewer GPU-hours than other approaches and 1000-fold less than \"standard\" NAS. On CIFAR-10, the ENAS design achieved a test error of 2.89%, comparable to NASNet. On Penn Treebank, the ENAS design reached test perplexity of 55.8.[8]\n",
            " Evolution[edit] An alternative approach to NAS is based on evolutionary algorithms, which has been employed by several groups.[9][10][11][12][13][14][15] An Evolutionary Algorithm for Neural Architecture Search generally performs the following procedure.[16] First a pool consisting of different candidate architectures along with their validation scores (fitness) is initialised. At each step the architectures in the candidate pool are mutated (e.g.: 3x3 convolution instead of a 5x5 convolution). Next the new architectures are trained from scratch for a few epochs and their validation scores are obtained. This is followed by replacing the lowest scoring architectures in the candidate pool with the better, newer architectures. This procedure is repeated multiple times and thus the candidate pool is refined over time. Mutations in the context of evolving ANNs are operations such as adding or removing a layer, which include changing the type of a layer (e.g., from convolution to pooling), changing the hyperparameters of a layer, or changing the training hyperparameters. On CIFAR-10 and ImageNet, evolution and RL performed comparably, while both slightly outperformed random search.[12][11]\n",
            " Bayesian optimization[edit] Bayesian Optimization (BO), which has proven to be an efficient method for hyperparameter optimization, can also be applied to NAS. In this context, the objective function maps an architecture to its validation error after being trained for a number of epochs. At each iteration, BO uses a surrogate to model this objective function based on previously obtained architectures and their validation errors. One then chooses the next architecture to evaluate by maximizing an acquisition function, such as expected improvement, which provides a balance between exploration and exploitation. Acquisition function maximization and objective function evaluation are often computationally expensive for NAS, and make the application of BO challenging in this context. Recently, BANANAS[17] has achieved promising results in this direction by introducing a high-performing instantiation of BO coupled to a neural predictor.\n",
            " Hill-climbing[edit] Another group used a hill climbing procedure that applies network morphisms, followed by short cosine-annealing optimization runs. The approach yielded competitive results, requiring resources on the same order of magnitude as training a single network. E.g., on CIFAR-10, the method designed and trained a network with an error rate below 5% in 12 hours on a single GPU.[18]\n",
            " Multi-objective search[edit] While most approaches solely focus on finding architecture with maximal predictive performance, for most practical applications other objectives are relevant, such as memory consumption, model size or inference time (i.e., the time required to obtain a prediction). Because of that, researchers created a multi-objective search.[15][19]\n",
            " LEMONADE[15] is an evolutionary algorithm that adopted Lamarckism to efficiently optimize multiple objectives. In every generation, child networks are generated to improve the Pareto frontier with respect to the current population of ANNs.\n",
            " Neural Architect[19] is claimed to be a resource-aware multi-objective RL-based NAS with network embedding and performance prediction. Network embedding encodes an existing network to a trainable embedding vector. Based on the embedding, a controller network generates transformations of the target network. A multi-objective reward function considers network accuracy, computational resource and training time. The reward is predicted by multiple performance simulation networks that are pre-trained or co-trained with the controller network. The controller network is trained via policy gradient. Following a modification, the resulting candidate network is evaluated by both an accuracy network and a training time network. The results are combined by a reward engine that passes its output back to the controller network.\n",
            " One-shot models[edit] RL or evolution-based NAS require thousands of GPU-days of searching training to achieve state-of-the-art computer vision results as described in the NASNet, mNASNet and MobileNetV3 papers.[4][20][21]\n",
            " To reduce computational cost, many recent NAS methods rely on the weight-sharing idea.[22][23] In this approach, a single overparameterized supernetwork (also known as the one-shot model) is defined. A supernetwork is a very large Directed Acyclic Graph (DAG) whose subgraphs are different candidate neural networks. Thus, in a supernetwork, the weights are shared among a large number of different sub-architectures that have edges in common, each of which is considered as a path within the supernet. The essential idea is to train one supernetwork that spans many options for the final design rather than generating and training thousands of networks independently. In addition to the learned parameters, a set of architecture parameters are learnt to depict preference for one module over another. Such methods reduce the required computational resources to only a few GPU days.\n",
            " More recent works further combine this weight-sharing paradigm, with a continuous relaxation of the search space,[24][25][26][27] which enables the use of gradient-based optimization methods. These approaches are generally referred to as differentiable NAS and have proven very efficient in exploring the search space of neural architectures. One of the most popular algorithms amongst the gradient-based methods for NAS is DARTS.[26] However, DARTS faces problems such as performance collapse due to an inevitable aggregation of skip connections and poor generalization which were tackled by many future algorithms.[28][29][30][31] Methods like [29][30] aim at robustifying DARTS and making the validation accuracy landscape smoother by introducing a Hessian norm based regularisation and random smoothing adversarial attack respectively. The cause of performance degradation is later analyzed from the architecture selection aspect.[32]\n",
            " Differentiable NAS has shown to produce competitive results using a fraction of the search-time required by RL-based search methods. For example, FBNet (which is short for Facebook Berkeley Network) demonstrated that supernetwork-based search produces networks that outperform the speed-accuracy tradeoff curve of mNASNet and MobileNetV2 on the ImageNet image-classification dataset. FBNet accomplishes this using over 400x less search time than was used for mNASNet.[33][34][35] Further, SqueezeNAS demonstrated that supernetwork-based NAS produces neural networks that outperform the speed-accuracy tradeoff curve of MobileNetV3 on the Cityscapes semantic segmentation dataset, and SqueezeNAS uses over 100x less search time than was used in the MobileNetV3 authors' RL-based search.[36][37]\n",
            " Neural architecture search benchmarks[edit] Neural architecture search often requires large computational resources, due to its expensive training and evaluation phases. This further leads to a large carbon footprint required for the evaluation of these methods.  To overcome this limitation, NAS benchmarks[38][39][40][41] have been introduced, from which one can either query or predict the final performance of neural architectures in seconds. A NAS benchmark is defined as a dataset with a fixed train-test split, a search space, and a fixed training pipeline (hyperparameters). There are primarily two types of NAS benchmarks: a surrogate NAS benchmark and a tabular NAS benchmark. A surrogate benchmark uses a  surrogate model (e.g.: a neural network) to predict the performance of an architecture from the search space. On the other hand, a tabular benchmark queries the actual performance of an architecture trained up to convergence. Both of these benchmarks are queryable and can be used to efficiently simulate many NAS algorithms using only a CPU to query the benchmark instead of training an architecture from scratch.\n",
            " See also[edit] Further reading[edit] Survey articles.\n",
            " References[edit]\n",
            "Number of unique words: 511\n",
            "Unique words: {'vision', 'na', 'ptb', 'curve', 'final', 'within', 'learningedit', 'object', 'fold', 'designed', 'claimed', 'considers', 'continuous', 'achieve', 'see', 'automating', 'estimation', 'employed', 'regularisation', 'proven', 'billion', 'comparable', 'norm', 'faster', 'vector', 'rely', 'content', 'return', 'learned', 'different', 'rather', 'referencesedit', 'also', 'researcher', 'humaninvented', 'initialised', 'improved', 'via', 'included', 'extent', 'dag', 'predicted', 'reached', 'single', 'framework', 'repeated', 'removing', 'frontier', 'day', 'varying', 'outperformed', 'evolutionbased', 'hand', 'generalization', 'alsoedit', 'resulting', 'readingedit', 'applies', 'handdesigned', 'factor', 'include', 'resource', 'penn', 'whose', 'epoch', 'subgraphs', 'generated', 'subarchitectures', 'paper', 'benchmarksedit', 'context', 'imagenet', 'generates', 'making', 'use', 'cotrained', 'searchedit', 'followed', 'initial', 'optimize', 'considered', 'cross', 'child', 'defined', 'instantiation', 'returned', 'pareto', 'produce', 'serve', 'targeting', 'socalled', 'dart', 'introducing', 'small', 'thousand', 'bit', 'created', 'combine', 'new', 'issue', 'survey', 'exploitation', 'method', 'predictor', 'independently', 'recent', 'smoothing', 'population', 'fewer', 'task', 'option', 'efficiently', 'coupled', 'reinforcement', 'lengthy', 'trainable', 'treebank', 'query', 'segmentation', 'edge', 'pretrained', 'lemonade', 'exceeded', 'connection', 'image', 'percent', 'tradeoff', 'cityscape', 'tabular', 'subgraph', 'requires', 'bayesian', 'mutation', 'path', 'described', 'newer', 'achieved', 'error', 'lamarckism', 'carbon', 'evaluated', 'demonstrated', 'group', 'generally', 'climbing', 'cell', 'lowest', 'detection', 'artificial', 'outperforms', 'architecture', 'essential', 'reaching', 'magnitude', 'target', 'queryable', 'one', 'leading', 'fasterrcnn', 'multiple', 'searching', 'relevant', 'depict', 'popular', 'studied', 'shared', 'standard', 'cause', 'closely', 'highperforming', 'convergence', 'generating', 'parameter', 'article', 'thus', 'degradation', 'semantic', 'simulation', 'modelsedit', 'due', 'normal', 'reward', 'recently', 'consumption', 'respectively', 'minimize', 'however', 'banana', 'face', 'weightsharing', 'prediction', 'best', 'benchmark', 'candidate', 'per', 'convoluting', 'convolution', 'traintest', 'transferred', 'automated', 'replacing', 'exploring', 'pooling', 'skip', 'poor', 'flopsa', 'split', 'exploration', 'practical', 'feature', 'canonical', 'character', 'application', 'stateoftheart', 'cpu', 'categorized', 'known', 'collapse', 'amongst', 'combined', 'fbnet', 'score', 'run', 'scoring', 'block', 'robustifying', 'work', 'berkeley', 'related', 'current', 'width', 'according', 'better', 'competitive', 'maximization', 'share', 'automl', 'along', 'paradigm', 'engine', 'field', 'overparameterized', 'step', 'model', 'focus', 'larger', 'iteration', 'maximal', 'continued', 'based', 'smoother', 'algorithm', 'transferring', 'acquisition', 'output', 'lstm', 'took', 'lower', 'differentiable', 'require', 'enas', 'accomplishes', 'neural', 'function', 'par', 'building', 'either', 'obtained', 'addressed', 'evolving', 'finding', 'reduced', 'top', 'refined', 'language', 'performed', 'entropy', 'hour', 'mobilenetv', 'enables', 'machine', 'subfield', 'ie', 'layer', 'previously', 'rate', 'order', 'gpudays', 'ha', 'directly', 'metalearning', 'gpu', 'existing', 'corresponding', 'changing', 'analyzed', 'policy', 'promising', 'predictive', 'referred', 'fraction', 'many', 'transformation', 'adding', 'footprint', 'cost', 'chooses', 'several', 'alternative', 'hill', 'design', 'adopted', 'integrated', 'make', 'challenging', 'prior', 'composed', 'searchtime', 'stacking', 'procedure', 'stride', 'computer', 'exceed', 'performance', 'expensive', 'zoph', 'space', 'evolutionedit', 'primarily', 'pool', 'every', 'objective', 'evaluation', 'preference', 'respect', 'hessian', 'improvement', 'validation', 'set', 'predict', 'test', 'modification', 'inevitable', 'relaxation', 'memory', 'aggregation', 'maximizes', 'pass', 'eg', 'generation', 'multiobjective', 'imageclassification', 'landscape', 'outperform', 'applied', 'back', 'second', 'rlbased', 'size', 'author', 'le', 'span', 'network', 'following', 'merge', 'oneshot', 'aim', 'search', 'common', 'accuracy', 'approach', 'optimization', 'controller', 'inference', 'scratch', 'type', 'x', 'hyperparameter', 'computational', 'performs', 'overcome', 'two', 'yielded', 'shown', 'often', 'embedding', 'direction', 'another', 'wa', 'mnasnet', 'comparably', 'introduced', 'maximizing', 'simulate', 'us', 'example', 'rl', 'instead', 'train', 'hyperparameters', 'like', 'height', 'reduction', 'gpuhours', 'idea', 'loss', 'evolution', 'using', 'encodes', 'short', 'number', 'discovers', 'technique', 'tackled', 'perplexity', 'process', 'solely', 'optimizationedit', 'gradient', 'underpin', 'select', 'supernet', 'system', 'lead', 'rival', 'bo', 'computationally', 'resourceaware', 'barret', 'nasnet', 'learnt', 'future', 'among', 'widely', 'higher', 'constrained', 'ann', 'expected', 'module', 'morphisms', 'manuallydesigned', 'training', 'selection', 'problem', 'reduce', 'obtain', 'limitation', 'computation', 'result', 'hillclimbingedit', 'cosineannealing', 'addition', 'used', 'viet', 'next', 'attack', 'adversarial', 'fixed', 'directed', 'optimal', 'modeling', 'later', 'learning', 'consisting', 'main', 'actual', 'acyclic', 'quoc', 'supernetwork', 'evolutionary', 'gradientbased', 'weight', 'architect', 'supernetworkbased', 'mutated', 'surrogate', 'squeezenas', 'convolutional', 'level', 'strategy', 'element', 'trained', 'facebook', 'coco', 'fitness', 'large', 'evaluate', 'requiring', 'anns', 'slightly', 'efficient', 'required', 'cifar', 'dataset', 'improve', 'speedaccuracy', 'classification', 'pipeline', 'graph', 'first', 'time', 'aspect', 'recurrent', 'phase', 'copy', 'map', 'random', 'provides', 'operation', 'input', 'balance'}\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "url = 'https://en.wikipedia.org/wiki/Neural_architecture_search'\n",
        "html_text = get_html_text(url)\n",
        "text = extract_text(html_text)\n",
        "cleaned_text = clean_data(text)\n",
        "normalized_text = normalize_text(cleaned_text)\n",
        "tokens = tokenize_text(normalized_text)\n",
        "lemmatized_tokens = lemmatize_text(tokens)\n",
        "filtered_tokens = remove_stopwords(lemmatized_tokens)\n",
        "unique_words = get_unique_words(filtered_tokens)\n",
        "\n",
        "print(\"Number of unique words:\", len(unique_words))\n",
        "print(\"Unique words:\", unique_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
