{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assi #3 FastText","metadata":{}},{"cell_type":"markdown","source":"### 1. Load Yelp Dataset\n### 2. Text Preprocessing\n### 3. Modeling\n### 4. Pretrained-FastText\n### 5. Load Annoy\n### 6. Extract Embeddings\n### 7. Find Similar Words\n### 8. Find Opposite Words\n### 9. Pretrained vs Scratch Train Comparison\n### 10. Extract CSV Table for Results","metadata":{}},{"cell_type":"markdown","source":"# Silvana Yacoub 20201091\n# Maher Mohsen   20200415","metadata":{}},{"cell_type":"code","source":"  \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-20T10:38:50.497210Z","iopub.execute_input":"2024-04-20T10:38:50.497495Z","iopub.status.idle":"2024-04-20T10:38:51.348572Z","shell.execute_reply.started":"2024-04-20T10:38:50.497469Z","shell.execute_reply":"2024-04-20T10:38:51.347690Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/Dataset_User_Agreement.pdf\n/kaggle/input/yelp_academic_dataset_review.json\n/kaggle/input/yelp_academic_dataset_checkin.json\n/kaggle/input/yelp_academic_dataset_business.json\n/kaggle/input/yelp_academic_dataset_tip.json\n/kaggle/input/yelp_academic_dataset_user.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Load Yelp Dataset","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\ndata_file = open(\"/kaggle/input/yelp_academic_dataset_tip.json\")\ndata = []\nfor line in data_file:\n    data.append(json.loads(line))\ntip_df = pd.DataFrame(data)\ndata_file.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:42:11.926779Z","iopub.execute_input":"2024-04-20T10:42:11.927626Z","iopub.status.idle":"2024-04-20T10:42:19.385918Z","shell.execute_reply.started":"2024-04-20T10:42:11.927592Z","shell.execute_reply":"2024-04-20T10:42:19.385126Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tip_df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:45:23.523717Z","iopub.execute_input":"2024-04-20T10:45:23.524560Z","iopub.status.idle":"2024-04-20T10:45:23.545759Z","shell.execute_reply.started":"2024-04-20T10:45:23.524530Z","shell.execute_reply":"2024-04-20T10:45:23.544950Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                       user_id             business_id  \\\n0       AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n1       NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n2       -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n3       FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n4       ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n...                        ...                     ...   \n908910  eYodOTF8pkqKPzHkcxZs-Q  3lHTewuKFt5IImbXJoFeDQ   \n908911  1uxtQAuJ2T5Xwa_wp7kUnA  OaGf0Dp56ARhQwIDT90w_g   \n908912  v48Spe6WEpqehsF2xQADpg  hYnMeAO77RGyTtIzUSKYzQ   \n908913  ckqKGM2hl7I9Chp5IpAhkw  s2eyoTuJrcP7I_XyjdhUHQ   \n908914  4tF1CWdMxvvwpUIgGsDygA  _cb1Vg1NIWry8UA0jyuXnQ   \n\n                                                     text  \\\n0                          Avengers time with the ladies.   \n1       They have lots of good deserts and tasty cuban...   \n2                  It's open even when you think it isn't   \n3                               Very decent fried chicken   \n4                  Appetizers.. platter special for lunch   \n...                                                   ...   \n908910              Disappointed in one of your managers.   \n908911                            Great food and service.   \n908912                                Love their Cubans!!   \n908913                            Great pizza great price   \n908914                  Food is good value but a bit hot!   \n\n                       date  compliment_count  \n0       2012-05-18 02:17:21                 0  \n1       2013-02-05 18:35:10                 0  \n2       2013-08-18 00:56:08                 0  \n3       2017-06-27 23:05:38                 0  \n4       2012-10-06 19:43:09                 0  \n...                     ...               ...  \n908910  2021-09-11 19:18:57                 0  \n908911  2021-10-30 11:54:36                 0  \n908912  2021-11-05 13:18:56                 0  \n908913  2021-11-20 16:11:44                 0  \n908914  2021-12-07 22:30:00                 0  \n\n[908915 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>text</th>\n      <th>date</th>\n      <th>compliment_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n      <td>Avengers time with the ladies.</td>\n      <td>2012-05-18 02:17:21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NBN4MgHP9D3cw--SnauTkA</td>\n      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n      <td>They have lots of good deserts and tasty cuban...</td>\n      <td>2013-02-05 18:35:10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-copOvldyKh1qr-vzkDEvw</td>\n      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n      <td>It's open even when you think it isn't</td>\n      <td>2013-08-18 00:56:08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FjMQVZjSqY8syIO-53KFKw</td>\n      <td>hV-bABTK-glh5wj31ps_Jw</td>\n      <td>Very decent fried chicken</td>\n      <td>2017-06-27 23:05:38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ld0AperBXk1h6UbqmM80zw</td>\n      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n      <td>Appetizers.. platter special for lunch</td>\n      <td>2012-10-06 19:43:09</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>908910</th>\n      <td>eYodOTF8pkqKPzHkcxZs-Q</td>\n      <td>3lHTewuKFt5IImbXJoFeDQ</td>\n      <td>Disappointed in one of your managers.</td>\n      <td>2021-09-11 19:18:57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>908911</th>\n      <td>1uxtQAuJ2T5Xwa_wp7kUnA</td>\n      <td>OaGf0Dp56ARhQwIDT90w_g</td>\n      <td>Great food and service.</td>\n      <td>2021-10-30 11:54:36</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>908912</th>\n      <td>v48Spe6WEpqehsF2xQADpg</td>\n      <td>hYnMeAO77RGyTtIzUSKYzQ</td>\n      <td>Love their Cubans!!</td>\n      <td>2021-11-05 13:18:56</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>908913</th>\n      <td>ckqKGM2hl7I9Chp5IpAhkw</td>\n      <td>s2eyoTuJrcP7I_XyjdhUHQ</td>\n      <td>Great pizza great price</td>\n      <td>2021-11-20 16:11:44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>908914</th>\n      <td>4tF1CWdMxvvwpUIgGsDygA</td>\n      <td>_cb1Vg1NIWry8UA0jyuXnQ</td>\n      <td>Food is good value but a bit hot!</td>\n      <td>2021-12-07 22:30:00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>908915 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Select only the 'text' column and convert it to a Series\ntext_series = tip_df['text']\n\n# Select a subset of rows (e.g., first 10000 rows)\ntext_subset_series = text_series.head(10000)\n\n# Convert the Series to a DataFrame with only one column\ntext_subset_df = pd.DataFrame({'text': text_subset_series})\n\n# Display the subset DataFrame\nprint(text_subset_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:45:36.789189Z","iopub.execute_input":"2024-04-20T10:45:36.789537Z","iopub.status.idle":"2024-04-20T10:45:36.801331Z","shell.execute_reply.started":"2024-04-20T10:45:36.789511Z","shell.execute_reply":"2024-04-20T10:45:36.800303Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"                                                   text\n0                        Avengers time with the ladies.\n1     They have lots of good deserts and tasty cuban...\n2                It's open even when you think it isn't\n3                             Very decent fried chicken\n4                Appetizers.. platter special for lunch\n...                                                 ...\n9995         We love the nachos and jerk chicken wings!\n9996           Mom, Dad, Sis, nephews, & us for brunch.\n9997                                          Meh. Dry.\n9998        Great latte. Extra hot, just how I like it!\n9999  Late flight to Dallas for conference in Dr Wor...\n\n[10000 rows x 1 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER, and word vectors\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\n# Function to preprocess text\ndef preprocess_text(text):\n    \n    # Remove punctuation and special characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    \n    # Remove leading/trailing white spaces and convert to lowercase\n    text = text.strip().lower()\n    \n    # Tokenize the text\n    tokens = nlp(text)\n    \n    # Lemmatize tokens and remove stopwords and non-alphabetic tokens\n    tokens = [token.lemma_ for token in tokens if token.lemma_ not in STOP_WORDS and token.is_alpha]\n    \n    # Join tokens back into a string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the 'text' column\ntext_subset_df['preprocessed_text'] = text_subset_df['text'].apply(preprocess_text)\n\n# Display the preprocessed text\nprint(text_subset_df['preprocessed_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:45:58.301881Z","iopub.execute_input":"2024-04-20T10:45:58.302468Z","iopub.status.idle":"2024-04-20T10:46:38.216658Z","shell.execute_reply.started":"2024-04-20T10:45:58.302439Z","shell.execute_reply":"2024-04-20T10:46:38.215712Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0                                       avenger time lady\n1                    lot good desert tasty cuban sandwich\n2                                              open think\n3                                    decent fried chicken\n4                         appetizer platter special lunch\n                              ...                        \n9995                        love nachos jerk chicken wing\n9996                           mom dad sis nephews brunch\n9997                                              meh dry\n9998                         great latte extra hot I like\n9999    late flight dallas conference dr worth bag fly...\nName: preprocessed_text, Length: 10000, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Modeling","metadata":{}},{"cell_type":"code","source":"from gensim.models import FastText\nfrom gensim.utils import simple_preprocess\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and validation sets\ntrain_data, valid_data = train_test_split(text_subset_df[['preprocessed_text']], test_size=0.2, random_state=42)\n\n# Tokenize the text data using Gensim's simple_preprocess\ntrain_corpus = [simple_preprocess(text) for text in train_data['preprocessed_text']]\nvalid_corpus = [simple_preprocess(text) for text in valid_data['preprocessed_text']]\n\n# Train the FastText model\nmodel = FastText(vector_size=100, window=10, min_count=1, workers=4)\nmodel.build_vocab(corpus_iterable=train_corpus)\nmodel.train(corpus_iterable=train_corpus, total_examples=len(train_corpus), epochs=1500)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:22:12.332409Z","iopub.execute_input":"2024-04-20T11:22:12.333045Z","iopub.status.idle":"2024-04-20T11:27:22.268715Z","shell.execute_reply.started":"2024-04-20T11:22:12.333010Z","shell.execute_reply":"2024-04-20T11:27:22.267778Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(61875750, 69090000)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. Pretrained-FastText","metadata":{}},{"cell_type":"code","source":"import fasttext\nfrom huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-et-vectors\", filename=\"model.bin\")\npretrained_model = fasttext.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:47:56.236994Z","iopub.execute_input":"2024-04-20T10:47:56.238039Z","iopub.status.idle":"2024-04-20T10:48:32.922465Z","shell.execute_reply.started":"2024-04-20T10:47:56.238005Z","shell.execute_reply":"2024-04-20T10:48:32.921453Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.bin:   0%|          | 0.00/7.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"662e8a784c314771b28e9ad7f1cdc660"}},"metadata":{}},{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport string\nfrom annoy import AnnoyIndex\n\nrandom_words = ['apple', 'banana', 'carrot', 'dog', 'elephant', 'flower', 'guitar', 'house', 'internet', 'jungle', 'kite', 'lion', 'mountain', 'notebook', 'orange', 'penguin', 'queen', 'rabbit', 'sunshine', 'tree']\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:48:39.937711Z","iopub.execute_input":"2024-04-20T10:48:39.938099Z","iopub.status.idle":"2024-04-20T10:48:40.312082Z","shell.execute_reply.started":"2024-04-20T10:48:39.938070Z","shell.execute_reply":"2024-04-20T10:48:40.310674Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 5. Load Annoy","metadata":{}},{"cell_type":"code","source":"from annoy import AnnoyIndex\nimport random\n\n# Annoy Index Setup\ndef setup_annoy_index(pretrained_model):\n    num_dims = pretrained_model.get_dimension()\n    annoy_index = AnnoyIndex(num_dims, 'angular')\n    for i, word in enumerate(pretrained_model.get_words()):\n        vector = pretrained_model.get_word_vector(word)\n        annoy_index.add_item(i, vector)\n    annoy_index.build(50)  # 50 trees for approximation\n    return annoy_index\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:48:41.559618Z","iopub.execute_input":"2024-04-20T10:48:41.560313Z","iopub.status.idle":"2024-04-20T10:48:41.566303Z","shell.execute_reply.started":"2024-04-20T10:48:41.560281Z","shell.execute_reply":"2024-04-20T10:48:41.565111Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 6. Extract Embeddings","metadata":{}},{"cell_type":"code","source":"# Word Vector Retrieval Functions\ndef get_embedding_vectors_pretrained(pretrained_model, words):\n    return [pretrained_model.get_word_vector(word) for word in words]\n\ndef get_embedding_vectors_model(model, words):\n    return [model.wv[word] for word in words]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:49:06.716858Z","iopub.execute_input":"2024-04-20T10:49:06.717194Z","iopub.status.idle":"2024-04-20T10:49:06.723071Z","shell.execute_reply.started":"2024-04-20T10:49:06.717169Z","shell.execute_reply":"2024-04-20T10:49:06.722104Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Call the function to get embedding vectors using the pretrained model\nembedding_vectors_pretrained = get_embedding_vectors_pretrained(pretrained_model, random_words)\n\n# Call the function to get embedding vectors using the model\nembedding_vectors_model = get_embedding_vectors_model(model, random_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:09:09.646751Z","iopub.execute_input":"2024-04-20T11:09:09.647698Z","iopub.status.idle":"2024-04-20T11:09:09.653074Z","shell.execute_reply.started":"2024-04-20T11:09:09.647666Z","shell.execute_reply":"2024-04-20T11:09:09.652123Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"embedding_vectors_pretrained[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:50:52.238494Z","iopub.execute_input":"2024-04-20T11:50:52.239327Z","iopub.status.idle":"2024-04-20T11:50:52.248784Z","shell.execute_reply.started":"2024-04-20T11:50:52.239295Z","shell.execute_reply":"2024-04-20T11:50:52.247698Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"array([-0.04744264,  0.00384141,  0.00681557,  0.04292644,  0.00821912,\n        0.00470234, -0.00639371, -0.01219297,  0.03871919,  0.10926136,\n        0.00069809, -0.01011722, -0.02791139,  0.04468519, -0.05492625,\n        0.05739029,  0.00486601, -0.08057441,  0.03488096,  0.00156933,\n        0.04467723,  0.01161048, -0.09750076,  0.05279666,  0.01010914,\n       -0.0185289 ,  0.08211946,  0.03969616, -0.02790017,  0.04107035,\n       -0.0087419 , -0.03521007,  0.05865506,  0.02087817,  0.04321015,\n        0.00992097, -0.04947798,  0.01697828, -0.05086547,  0.00774659,\n        0.00903939, -0.00128553, -0.05244497, -0.06282768,  0.03611057,\n       -0.02365493, -0.02683609, -0.01017902, -0.00073479, -0.04777697,\n        0.01084739,  0.05563303, -0.06064871,  0.0955272 ,  0.02082256,\n        0.01266333, -0.11118156, -0.02926121, -0.0423243 ,  0.01441076,\n        0.0282505 ,  0.02537795,  0.01983598, -0.0151052 , -0.00106145,\n        0.02459156, -0.01237434, -0.03405061,  0.01224788, -0.01393567,\n        0.01306086, -0.01345843,  0.01508326, -0.03071709, -0.00071688,\n        0.03247704, -0.08225954, -0.02548756,  0.07430397, -0.02006275,\n       -0.03594366,  0.02715967,  0.00781859, -0.05056211, -0.02015268,\n       -0.00927308,  0.07512894, -0.06240012,  0.07269753, -0.104437  ,\n       -0.04146105, -0.05376863, -0.05676252,  0.07160311,  0.06015052,\n       -0.05213708,  0.06621557, -0.02588248,  0.11444059, -0.01901934,\n        0.03906266, -0.01774851, -0.11794639,  0.04484953, -0.04475109,\n       -0.11546379, -0.02849415, -0.02636937, -0.08278091, -0.02889998,\n       -0.02885929,  0.00656032,  0.00743203, -0.0016703 ,  0.03566461,\n        0.03733245,  0.03600153, -0.01127219,  0.02665836, -0.06130975,\n        0.05248732,  0.00490629,  0.02875504, -0.05454843,  0.00195413,\n        0.05689535, -0.08148783,  0.03377277,  0.0416274 , -0.05142633,\n       -0.03787371,  0.00495021,  0.00217882, -0.04145631, -0.01364997,\n        0.03599429, -0.00450748, -0.01516889, -0.01310817, -0.05485437,\n       -0.01633435, -0.03061057,  0.00541635,  0.00965757,  0.02283518,\n        0.02853875,  0.01060723, -0.01385244, -0.00623081,  0.0180939 ,\n       -0.09972273, -0.06496249, -0.0445844 , -0.00081624,  0.00843995,\n        0.04644397,  0.06391906, -0.04403937, -0.05419046,  0.03350861,\n        0.02373856,  0.03813093, -0.05179831, -0.00800318, -0.00339501,\n       -0.04568861,  0.00117965, -0.01408727,  0.06727339, -0.10719936,\n        0.03966452,  0.03249058,  0.04392181, -0.02175782,  0.03723676,\n        0.01077349, -0.00820652,  0.01331867, -0.03362268, -0.03299123,\n        0.01234463,  0.00994936,  0.02438051,  0.05402601, -0.06820444,\n        0.01437567, -0.00842241,  0.0248435 , -0.00514592,  0.00103626,\n       -0.00206062, -0.0031368 ,  0.02653033, -0.02665707, -0.02810895,\n       -0.01779506,  0.00692187,  0.04088293,  0.03299163,  0.07967881,\n        0.01910441,  0.00649973, -0.02198591,  0.03085988,  0.0004114 ,\n        0.07867374,  0.04239751, -0.01268356, -0.02515448, -0.05806623,\n        0.02085246, -0.09016577,  0.00410204, -0.04517177, -0.01834926,\n       -0.01275606, -0.05055344,  0.06072959,  0.03095371, -0.03266051,\n        0.00564315, -0.01155499,  0.0006504 ,  0.01663428,  0.04172543,\n       -0.03358851,  0.03231056,  0.03854582, -0.02679344,  0.06143897,\n       -0.00676753,  0.06969184, -0.02645803,  0.05636267, -0.00894706,\n       -0.04936758, -0.00017829, -0.06443915, -0.03693692, -0.06188372,\n        0.04518006, -0.07850596, -0.02741001, -0.0715623 , -0.05058196,\n       -0.02824482, -0.01248896,  0.02767389, -0.02171879,  0.00167804,\n        0.00911042, -0.00228551, -0.00250831, -0.0484406 , -0.01487194,\n        0.03086053, -0.00438679, -0.02329532, -0.03703518,  0.00890034,\n       -0.03733176, -0.04102652, -0.0136836 ,  0.02537568, -0.01406111,\n        0.034704  , -0.03859921, -0.08312044,  0.02346133,  0.04978033,\n       -0.05772374, -0.05877975, -0.02417387, -0.06782231, -0.00712793,\n       -0.07058255,  0.00784789, -0.07354023,  0.04163436,  0.06728294,\n        0.00383498, -0.06206394,  0.02327587,  0.0163078 ,  0.01773069,\n        0.00886432,  0.06374888, -0.0182792 , -0.01019504, -0.03668234,\n        0.03781804, -0.05985521,  0.01540245, -0.06468835,  0.00996193,\n        0.0673063 ,  0.00648268,  0.00605619,  0.01577675, -0.03081161],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"embedding_vectors_model[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:50:56.028911Z","iopub.execute_input":"2024-04-20T11:50:56.029745Z","iopub.status.idle":"2024-04-20T11:50:56.036891Z","shell.execute_reply.started":"2024-04-20T11:50:56.029716Z","shell.execute_reply":"2024-04-20T11:50:56.035875Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"array([-1.7426789 ,  1.3343756 ,  1.4989388 , -3.3512259 ,  0.1364948 ,\n        1.2998782 ,  1.5213771 ,  3.2781029 , -0.24570261,  2.0123072 ,\n       -5.4301324 ,  0.5975554 , -2.4727957 ,  2.1775308 , -2.848344  ,\n        3.012284  ,  1.3892561 , -5.005315  , -2.7365255 ,  1.6266829 ,\n        2.9624965 , -3.1665933 ,  3.8534505 , -3.722627  , -0.17431489,\n       -0.753417  , -0.37247497, -0.92289835,  2.2489753 , -0.07911886,\n       -3.8998075 ,  3.303678  ,  3.94861   ,  1.9617566 ,  1.7799965 ,\n       -2.0607963 , -0.35215387,  1.2693568 ,  3.9579241 , -1.6779844 ,\n       -1.4867896 ,  1.3114182 ,  0.28707972, -1.4058387 , -0.9827716 ,\n       -0.13998191, -2.3808253 ,  1.5548446 , -2.0455868 , -0.8498095 ,\n        5.9399014 ,  4.671759  ,  4.89676   ,  2.291641  ,  3.0579643 ,\n       -1.4225575 ,  2.3422408 , -3.9113786 ,  2.1370952 , -0.22201627,\n       -5.6238914 , -0.33667004, -0.06739493,  1.9262583 , -0.5009882 ,\n        4.112852  ,  1.1300583 ,  4.292957  , -0.60009676, -1.7598614 ,\n       -1.8590516 ,  2.08578   , -2.320772  , -0.19529527,  0.07451153,\n       -2.2992384 ,  3.931608  , -1.471059  ,  0.8734918 , -5.5032926 ],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"## 7. Find Similar Words","metadata":{}},{"cell_type":"code","source":"# Word Similarity Functions\ndef find_similar_words_pretrained(pretrained_model, word, topn=10):\n    return pretrained_model.get_nearest_neighbors(word, k=topn)\n\ndef find_similar_words_model(model, word, topn=10):\n    return model.wv.most_similar(word, topn=topn)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:09:11.482469Z","iopub.execute_input":"2024-04-20T11:09:11.482830Z","iopub.status.idle":"2024-04-20T11:09:11.488163Z","shell.execute_reply.started":"2024-04-20T11:09:11.482791Z","shell.execute_reply":"2024-04-20T11:09:11.487155Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 8. Find Opposite Words","metadata":{}},{"cell_type":"code","source":"# Word Opposite Functions\ndef find_opposite_words_pretrained(pretrained_model, annoy_index, word, topn=10):\n    word_vector = pretrained_model.get_word_vector(word)\n    similar_indices = annoy_index.get_nns_by_vector(-word_vector, topn, include_distances=True)\n    opposite_words = [(pretrained_model.get_words()[idx], 1 - distance) for idx, distance in zip(similar_indices[0], similar_indices[1])]\n    return opposite_words\n\ndef find_opposite_words_model(model, word, topn=10):\n    opposite_words = model.wv.most_similar(negative=[word], topn=topn)\n    return opposite_words","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:09:11.763329Z","iopub.execute_input":"2024-04-20T11:09:11.763638Z","iopub.status.idle":"2024-04-20T11:09:11.769685Z","shell.execute_reply.started":"2024-04-20T11:09:11.763612Z","shell.execute_reply":"2024-04-20T11:09:11.768842Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## 9. Pretrained vs Scratch Train Comparison","metadata":{}},{"cell_type":"code","source":"# Main Function\ndef compare_models(pretrained_model, model):\n    \n    # Setup Annoy Index\n    annoy_index = setup_annoy_index(pretrained_model)\n    \n    # Find similar and opposite words for each random word using pretrained model\n    similar_words_dict_pretrained = {}\n    opposite_words_dict_pretrained = {}\n    for word in random_words:\n        similar_words_dict_pretrained[word] = find_similar_words_pretrained(pretrained_model, word)\n        opposite_words_dict_pretrained[word] = find_opposite_words_pretrained(pretrained_model, annoy_index, word)\n    \n    # Find similar and opposite words for each random word using model\n    similar_words_dict_model = {}\n    opposite_words_dict_model = {}\n    for word in random_words:\n        similar_words_dict_model[word] = find_similar_words_model(model, word)\n        opposite_words_dict_model[word] = find_opposite_words_model(model, word)\n    \n    # Print the results for both models\n    for word in random_words:\n        print(f\"Word: {word}\\n\")\n        \n        print(\"Similar Words (Pretrained Model):\")\n        for similar_word, similarity_score in similar_words_dict_pretrained[word]:\n            print(f\"{similar_word}: {similarity_score}\")\n        print(\"\\n\")\n        \n        print(\"Opposite Words (Pretrained Model):\")\n        for opposite_word, opposite_score in opposite_words_dict_pretrained[word]:\n            print(f\"{opposite_word}: {opposite_score}\")\n        print(\"\\n\")\n        \n        print(\"Similar Words (Model):\")\n        for similar_word, similarity_score in similar_words_dict_model[word]:\n            print(f\"{similar_word}: {similarity_score}\")\n        print(\"\\n\")\n        \n        print(\"Opposite Words (Model):\")\n        for opposite_word, opposite_score in opposite_words_dict_model[word]:\n            print(f\"{opposite_word}: {opposite_score}\")\n        print(\"\\n\")\n        print(\"-------------------------------------------------------------------\")\n    return similar_words_dict_pretrained, opposite_words_dict_pretrained, similar_words_dict_model, opposite_words_dict_model","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:14:59.107989Z","iopub.execute_input":"2024-04-20T12:14:59.108347Z","iopub.status.idle":"2024-04-20T12:14:59.118419Z","shell.execute_reply.started":"2024-04-20T12:14:59.108321Z","shell.execute_reply":"2024-04-20T12:14:59.117237Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Call the main function\nsimilar_words_dict_pretrained, opposite_words_dict_pretrained, similar_words_dict_model, opposite_words_dict_model = compare_models(pretrained_model, model)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:15:07.595877Z","iopub.execute_input":"2024-04-20T12:15:07.596248Z","iopub.status.idle":"2024-04-20T12:22:35.160310Z","shell.execute_reply.started":"2024-04-20T12:15:07.596220Z","shell.execute_reply":"2024-04-20T12:22:35.159369Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Word: apple\n\nSimilar Words (Pretrained Model):\n0.6745341420173645: Snapple\n0.6637983918190002: appleyard\n0.6263399124145508: apples\n0.6080800890922546: applespoti\n0.6041028499603271: applet\n0.5897731184959412: Nipple\n0.5882380604743958: applespot\n0.5674155354499817: apple-lisa\n0.5537776350975037: Rypple\n0.5484605431556702: Apple\n\n\nOpposite Words (Pretrained Model):\nQB1: -0.15393579006195068\nβ0: -0.15540635585784912\n80,7.: -0.16572999954223633\nD-x: -0.1677711009979248\nÜPK: -0.17205655574798584\nEÕSi: -0.17626047134399414\nZipfi: -0.17942500114440918\nq0: -0.17958998680114746\nERHS: -0.18480980396270752\nEw: -0.18548107147216797\n\n\nSimilar Words (Model):\napplebee: 0.7106369137763977\napply: 0.6901652216911316\nscrapple: 0.6876152157783508\nripple: 0.636448860168457\npineapple: 0.558565616607666\nappt: 0.515812337398529\ntriple: 0.4471432864665985\napproval: 0.4225643575191498\napplicable: 0.40602049231529236\nample: 0.3747911751270294\n\n\nOpposite Words (Model):\ngs: 0.3849387764930725\ncustomize: 0.3403310477733612\nstepup: 0.30737003684043884\nsteve: 0.30572444200515747\nyesterday: 0.2908530533313751\neh: 0.2898821532726288\nstencil: 0.28390029072761536\ncustomer: 0.2812068462371826\ninterested: 0.2779231369495392\ncourse: 0.2764214277267456\n\n\n-------------------------------------------------------------------\nWord: banana\n\nSimilar Words (Pretrained Model):\n0.7914311289787292: manana\n0.7566874623298645: Melt-Banana\n0.7355301380157471: Manana\n0.7030456066131592: Tanana\n0.6958869695663452: Bealanana\n0.672390341758728: Sanana\n0.6630967259407043: Rienana\n0.6579098105430603: Atsinanana\n0.6514954566955566: Banana\n0.6477656960487366: znana\n\n\nOpposite Words (Pretrained Model):\nÕKE: -0.05592954158782959\n.b: -0.07534337043762207\n0573: -0.08743548393249512\nlg3: -0.09337794780731201\nG77: -0.10044360160827637\nSNiP: -0.10055053234100342\neTI: -0.10330426692962646\n0555: -0.10677945613861084\n8206: -0.1081087589263916\np.3: -0.10812461376190186\n\n\nSimilar Words (Model):\nbananas: 0.9063526391983032\nana: 0.6961770057678223\ndana: 0.5655370354652405\nhana: 0.55150306224823\nalana: 0.5482808947563171\nnirvana: 0.5235306620597839\ntijuana: 0.37811875343322754\nwcna: 0.3435155153274536\ndivine: 0.3433588743209839\nbanish: 0.3418789207935333\n\n\nOpposite Words (Model):\ncarpaccio: 0.30689290165901184\nopi: 0.2907237708568573\nround: 0.2813935875892639\neasy: 0.28107061982154846\nstud: 0.27839016914367676\ninformative: 0.27801069617271423\nstock: 0.27664852142333984\nfitness: 0.275239497423172\ncorrespond: 0.2734493315219879\neuro: 0.2721787095069885\n\n\n-------------------------------------------------------------------\nWord: carrot\n\nSimilar Words (Pretrained Model):\n0.7893547415733337: parrot\n0.7397650480270386: Barcarrotas\n0.7300215363502502: ParrotBebop\n0.7256506681442261: Puchowe\n0.7185091972351074: MDMA-d.Parrott\n0.7086787223815918: garrote\n0.7016239762306213: Parrott\n0.6972708702087402: Scaparrott\n0.6936633586883545: carroures\n0.6739522218704224: AndrewsAngelinaAransasArcherArmstrongAtascosaAustinBaileyBanderaBastropBaylorBeeBellBexarBlancoBordenBosqueBowieBrazoriaBrazosBrewsterBriscoeBrooksBrownBurlesonBurnetCaldwellCalhounCallahanCameronCampCarsonCassCastroChambersCherokeeChildressClayCochranCokeColemanCollinCollingsworthColoradoComalComancheConchoCookeCoryellCottleCraneCrockettCrosbyCulbersonDallamDallasDawsonDeaf\n\n\nOpposite Words (Pretrained Model):\nVõtu: 0.19131404161453247\nM.T: 0.17597132921218872\nFPK: 0.1474490761756897\n: 0.13269835710525513\nJü: 0.13246625661849976\nBepco: 0.13216561079025269\nÜm: 0.12862491607666016\n.P: 0.12801194190979004\nKöö: 0.12583321332931519\nbib: 0.12307339906692505\n\n\nSimilar Words (Model):\ncarre: 0.6290273070335388\narroz: 0.5051280856132507\ncarrier: 0.5004098415374756\nrot: 0.44148480892181396\ncarte: 0.42225509881973267\ncart: 0.4080905616283417\narrow: 0.372463583946228\ncarl: 0.36717528104782104\nnaked: 0.36140862107276917\ncarman: 0.3594096899032593\n\n\nOpposite Words (Model):\nwawa: 0.3868132531642914\nhopefully: 0.34140998125076294\nbull: 0.3355679214000702\noar: 0.31099849939346313\nsubpar: 0.30329474806785583\nraunchy: 0.3010871708393097\nwa: 0.30055099725723267\nsushiand: 0.292047917842865\nfamiliar: 0.2919614017009735\nill: 0.2881183922290802\n\n\n-------------------------------------------------------------------\nWord: dog\n\nSimilar Words (Pretrained Model):\n0.7132166624069214: hot\n0.557005763053894: food\n0.5088931322097778: sugar\n0.5069419741630554: sucks\n0.5054174661636353: fucks\n0.5008330941200256: slutty\n0.49334049224853516: lazy\n0.48793452978134155: hairy\n0.4862593710422516: drink\n0.48344799876213074: jumps\n\n\nOpposite Words (Pretrained Model):\nesialgsena: -0.24599742889404297\neesrindlikus: -0.24708318710327148\nosalisena: -0.24887394905090332\nebatäielikus: -0.24926960468292236\nkahebrigaadilisena: -0.25385499000549316\nautoriteedina: -0.2561361789703369\nfüüsikalisena: -0.2571592330932617\naistitavana: -0.26220476627349854\ntemaatilisena: -0.2628662586212158\nkosmilises: -0.26600611209869385\n\n\nSimilar Words (Model):\ndoggy: 0.6759560108184814\nog: 0.6216485500335693\ndoggie: 0.5766898989677429\nfog: 0.5322515368461609\ndoc: 0.4345623254776001\nlog: 0.4324221909046173\njog: 0.42359068989753723\nuc: 0.39166682958602905\ndock: 0.39002570509910583\ndot: 0.3872806131839752\n\n\nOpposite Words (Model):\noriginal: 0.3384212553501129\nremoverweird: 0.32670655846595764\nparfait: 0.32635852694511414\ncoverage: 0.3188549876213074\nhot: 0.3136373460292816\nbeverage: 0.3123011887073517\nslice: 0.31105342507362366\nbullshit: 0.3082267642021179\norigin: 0.30461564660072327\nremodel: 0.29383355379104614\n\n\n-------------------------------------------------------------------\nWord: elephant\n\nSimilar Words (Pretrained Model):\n0.8207817673683167: elephants\n0.8018065094947815: Elephant\n0.7478556036949158: elephantopus\n0.7231231331825256: Elephants\n0.6948093771934509: Eléphant\n0.6641665101051331: Aphantochroa\n0.6421049237251282: decaying\n0.6417827010154724: wood-decaying\n0.6341092586517334: towels\n0.6319935321807861: Elephantine\n\n\nOpposite Words (Pretrained Model):\nEVC: -0.0653921365737915\nÄgli: -0.08342587947845459\nQSL: -0.08671867847442627\nMiy: -0.0876302719116211\neSLi: -0.08882761001586914\n珍島郡: -0.08953213691711426\nKMi: -0.09112882614135742\nGä: -0.09306848049163818\nHitu: -0.09784138202667236\nODUs: -0.09840166568756104\n\n\nSimilar Words (Model):\nelegant: 0.6158533692359924\nthanno: 0.4602467715740204\nbaron: 0.4403114914894104\ncsk: 0.4376955032348633\nstomache: 0.4084640145301819\nthanks: 0.40609556436538696\ninashannon: 0.3897039592266083\nstomach: 0.38866063952445984\ndifficult: 0.3771139085292816\nstephanie: 0.3701170086860657\n\n\nOpposite Words (Model):\nsalsa: 0.38263630867004395\nmixed: 0.35208261013031006\nsolitary: 0.34122103452682495\nsalsayum: 0.3368230164051056\nmarvelous: 0.33362022042274475\ndairy: 0.31422150135040283\nmiami: 0.3093203902244568\nsalsas: 0.3092062771320343\nmichigan: 0.3010368049144745\nbalsamic: 0.29074826836586\n\n\n-------------------------------------------------------------------\nWord: flower\n\nSimilar Words (Pretrained Model):\n0.8891685605049133: wildflower\n0.8398065567016602: Blower\n0.8306849002838135: Bellflower\n0.830608069896698: ReyMayflower\n0.8285796046257019: sunflower\n0.8236598372459412: blower\n0.815642237663269: Sunflower\n0.8094305396080017: Wildflower\n0.7995984554290771: Flower\n0.7986035346984863: Moonflower\n\n\nOpposite Words (Pretrained Model):\nloa-: -0.07092499732971191\n54-: -0.08224070072174072\nprg: -0.09646284580230713\np.3: -0.09828150272369385\nLOVL: -0.10723876953125\nlg3: -0.11128950119018555\nyelp: -0.11342215538024902\nTTTS: -0.11422884464263916\n.72: -0.11565279960632324\nPHP4: -0.11635875701904297\n\n\nSimilar Words (Model):\nsunflower: 0.8269307017326355\nlower: 0.7477952837944031\ncauliflower: 0.7222604155540466\ntower: 0.6114489436149597\nfollower: 0.5612549185752869\nflow: 0.5557513236999512\npower: 0.5421472191810608\nshower: 0.5258951187133789\nflora: 0.5061072707176208\nfloral: 0.46414732933044434\n\n\nOpposite Words (Model):\ndeclan: 0.3849359452724457\nbudino: 0.31645312905311584\nhibachi: 0.3111969828605652\nhasselhoffe: 0.3088700771331787\ndeck: 0.29965510964393616\nlittle: 0.29217207431793213\nsimilar: 0.2908208668231964\nmotivation: 0.27821293473243713\nasopada: 0.27194955945014954\nsignificantly: 0.2719019651412964\n\n\n-------------------------------------------------------------------\nWord: guitar\n\nSimilar Words (Pretrained Model):\n0.7674224376678467: guitars\n0.6755205988883972: drums\n0.6471867561340332: Guitar\n0.6326616406440735: Kitar\n0.6279263496398926: Sitar\n0.6109027862548828: goldrush\n0.610867977142334: sitar\n0.608880341053009: guitarra\n0.6026626825332642: vocal\n0.6015963554382324: swing\n\n\nOpposite Words (Pretrained Model):\n.72: -0.14033830165863037\nlg3: -0.1446300745010376\nELs: -0.14467835426330566\n7916: -0.1502777338027954\nvдhem: -0.1531006097793579\np.3: -0.15400004386901855\nFIEt: -0.15685689449310303\nprg: -0.1609506607055664\n140st: -0.1685115098953247\nSRKS: -0.17228293418884277\n\n\nSimilar Words (Model):\nguitarist: 0.5789699554443359\ntar: 0.45009973645210266\nohmygoodness: 0.3971058130264282\ntartar: 0.3959490656852722\noar: 0.392498642206192\nupdo: 0.3776964247226715\ngoodness: 0.3762364387512207\nrestrictionsetc: 0.37047749757766724\ngooda: 0.3608808219432831\nsauvignon: 0.35936570167541504\n\n\nOpposite Words (Model):\nnod: 0.30929622054100037\nearl: 0.29648110270500183\nbibimbop: 0.2834521234035492\nbudget: 0.28249281644821167\ngranola: 0.2707083225250244\nbudgetsavvy: 0.26814600825309753\noffice: 0.2662099301815033\nujs: 0.26458966732025146\neyebrow: 0.261106014251709\nbudino: 0.258516401052475\n\n\n-------------------------------------------------------------------\nWord: house\n\nSimilar Words (Pretrained Model):\n0.7571181654930115: deep-house\n0.7199534773826599: art-house\n0.712556779384613: Icehouse\n0.7072818875312805: Dochouse\n0.7011234760284424: warehouse\n0.6888847351074219: Epic-house\n0.6888394355773926: spouse\n0.6886950135231018: Hothouse\n0.6880742311477661: Henhouse\n0.684495747089386: farmhouse\n\n\nOpposite Words (Pretrained Model):\nK-89: -0.21223580837249756\nL.-il: -0.212976336479187\nPdLi: -0.21347415447235107\n35.1: -0.21572983264923096\nLVŽ: -0.21716129779815674\nμM: -0.2259693145751953\nXP31: -0.22791385650634766\nHÕ: -0.23140978813171387\n3-lt: -0.23172378540039062\n9,3.: -0.23308682441711426\n\n\nSimilar Words (Model):\nmadhouse: 0.7651577591896057\ninhouse: 0.7030497193336487\nbrewhouse: 0.6846993565559387\ndouse: 0.673374354839325\ntownhouse: 0.633041262626648\nsteakhouse: 0.6236541867256165\nhousekeeper: 0.6212711930274963\nrittenhouse: 0.6101171374320984\nsmokehouse: 0.6021925210952759\nmouse: 0.56609708070755\n\n\nOpposite Words (Model):\nplan: 0.3216434121131897\nanytime: 0.3135904371738434\nmile: 0.30453068017959595\nallan: 0.30432620644569397\ncilantro: 0.28714486956596375\nsignificantly: 0.2769109010696411\nlane: 0.27521899342536926\ndeath: 0.27511298656463623\nbras: 0.27066704630851746\nmiracle: 0.269016295671463\n\n\n-------------------------------------------------------------------\nWord: internet\n\nSimilar Words (Pretrained Model):\n0.7881157398223877: m-internet\n0.7535822987556458: TVinternet\n0.7526796460151672: Internet\n0.7516708374023438: internett\n0.7391056418418884: XInternet\n0.7079346179962158: Koduinternet\n0.6881517171859741: nternet\n0.6687052845954895: interneta\n0.6588441133499146: internetivõrk\n0.6453970670700073: internet.emt.ee\n\n\nOpposite Words (Pretrained Model):\nPatxi: -0.14622914791107178\nOsc: -0.1556018590927124\nAgca: -0.15952587127685547\nBayó: -0.16739845275878906\n白河市: -0.1690281629562378\nÓkér: -0.16939032077789307\nigl: -0.1725614070892334\nباميان: -0.1728144884109497\nSäiz: -0.17420101165771484\n58-3: -0.1744455099105835\n\n\nSimilar Words (Model):\nintern: 0.8777721524238586\ninternal: 0.851165235042572\ninternational: 0.7239200472831726\ninteract: 0.7228350639343262\ninterior: 0.7117586135864258\ninterpret: 0.6514018774032593\ninterview: 0.6441099047660828\ninterpretation: 0.5623717904090881\nintense: 0.5168554186820984\nlantern: 0.5079345107078552\n\n\nOpposite Words (Model):\nlaura: 0.30877816677093506\ncrabsjust: 0.3047986328601837\nlack: 0.30075007677078247\nbartendress: 0.29503920674324036\nmargarhita: 0.2940977215766907\nsalsas: 0.2877568304538727\nlas: 0.28330597281455994\ncrabmeat: 0.2805766463279724\nmoney: 0.2764021158218384\nplateit: 0.2732268273830414\n\n\n-------------------------------------------------------------------\nWord: jungle\n\nSimilar Words (Pretrained Model):\n0.6992387771606445: Bungle\n0.6514163017272949: jangle\n0.6229740381240845: Jangle\n0.6134817004203796: endAngle\n0.6077103018760681: Jungle\n0.5999616980552673: angle\n0.5980309247970581: mangle\n0.5821893215179443: Tangle\n0.5785723924636841: Dangle\n0.5767992734909058: triangle\n\n\nOpposite Words (Pretrained Model):\n6116: -0.19978857040405273\n42a: -0.22002530097961426\n7318: -0.22164344787597656\n4766: -0.22677600383758545\n8913: -0.22723066806793213\nTPIK: -0.22984302043914795\n7431: -0.2298828363418579\n37.a.: -0.23049890995025635\n0280: -0.2306652069091797\n65609: -0.23310768604278564\n\n\nSimilar Words (Model):\njingle: 0.7039547562599182\nsingle: 0.6294785141944885\ndingle: 0.612155020236969\ndangle: 0.5788987278938293\neagle: 0.48867979645729065\nsingletary: 0.4521428048610687\ngoogle: 0.4422081410884857\nstruggle: 0.4301433861255646\nyuengle: 0.42986199259757996\nsnuggle: 0.3917222321033478\n\n\nOpposite Words (Model):\nbenefit: 0.3086535632610321\npad: 0.28498655557632446\nwinebytheglass: 0.2846349775791168\nbenedict: 0.2826131284236908\nturkey: 0.27967509627342224\nmeat: 0.2782750129699707\nbonchon: 0.27751922607421875\nnotice: 0.27541154623031616\nwealth: 0.2675248682498932\nweisswurst: 0.26738452911376953\n\n\n-------------------------------------------------------------------\nWord: kite\n\nSimilar Words (Pretrained Model):\n0.8719635605812073: Olędzkite\n0.7691652178764343: orkite\n0.7641305327415466: padonokite\n0.7606732845306396: Melkite\n0.7552414536476135: Rupskite\n0.7534875869750977: Mirskite\n0.7531940937042236: rupskite\n0.7524303197860718: Gurskite\n0.7522873878479004: roguskite\n0.749453067779541: urukite\n\n\nOpposite Words (Pretrained Model):\n1i: -0.1504753828048706\nnr17: -0.18979573249816895\n8773: -0.19003748893737793\nõ-a: -0.1955881118774414\nÜSK: -0.19861257076263428\n4N: -0.20147466659545898\npW: -0.20545721054077148\ntq: -0.2121272087097168\ntjo: -0.214269757270813\ne7: -0.21452271938323975\n\n\nSimilar Words (Model):\nmidnite: 0.6575947403907776\nrite: 0.6497324109077454\nnite: 0.6455714702606201\naphrodite: 0.6104422211647034\nsite: 0.5853352546691895\nnewwhite: 0.5752695202827454\nte: 0.5709153413772583\nspite: 0.5705944895744324\nbite: 0.5460823774337769\nlite: 0.5367244482040405\n\n\nOpposite Words (Model):\ndecaf: 0.31703415513038635\neggdippe: 0.3139307200908661\npt: 0.30237090587615967\nibx: 0.29953375458717346\nucp: 0.2918338179588318\nugh: 0.2900049686431885\npay: 0.2875126600265503\nchuck: 0.2840132415294647\nshape: 0.2821580767631531\nhot: 0.27964699268341064\n\n\n-------------------------------------------------------------------\nWord: lion\n\nSimilar Words (Pretrained Model):\n0.7368613481521606: Clion\n0.7261545062065125: Ecthalion\n0.7260317802429199: elion\n0.7259875535964966: Ilion\n0.7163397073745728: Glion\n0.6956464648246765: ant-lion\n0.6922463774681091: Actelion\n0.6877405643463135: Hélion\n0.6855167746543884: Criquielion\n0.6832044124603271: Antlion\n\n\nOpposite Words (Pretrained Model):\n-loa: -0.18199050426483154\nC3V: -0.20266151428222656\nVÕT-i: -0.2078261375427246\nSNiP: -0.2097684144973755\nLTRO: -0.21237754821777344\nопРЭБ: -0.21597957611083984\nVLI: -0.21701490879058838\n7916: -0.2202078104019165\nVCIOMi: -0.22186434268951416\nAR-s: -0.22249555587768555\n\n\nSimilar Words (Model):\npavilion: 0.721034049987793\nexecution: 0.6427847146987915\nmillion: 0.6366864442825317\nlibation: 0.6297413110733032\ncaution: 0.6185544729232788\nwcaution: 0.614734947681427\nnotion: 0.605724573135376\ndillion: 0.6054475903511047\nliquidation: 0.5964431762695312\nlotion: 0.5954003930091858\n\n\nOpposite Words (Model):\nhype: 0.3428804278373718\nlawn: 0.30682796239852905\njerk: 0.3056303858757019\nlamp: 0.2952122390270233\njump: 0.28051528334617615\nomega: 0.2748367190361023\nhyper: 0.27420008182525635\nsinfully: 0.273870587348938\nslam: 0.27374571561813354\nlsu: 0.26882800459861755\n\n\n-------------------------------------------------------------------\nWord: mountain\n\nSimilar Words (Pretrained Model):\n0.8935726284980774: fountain\n0.8670065402984619: mountains\n0.86318039894104: 20mountains\n0.8240149617195129: mountain.ru\n0.7901449203491211: Fountains\n0.7881560921669006: cycling-mountain-bike\n0.7791293859481812: Mountain\n0.7560893297195435: Mountains\n0.7538598775863647: Mountaineers\n0.7392075061798096: Fountain\n\n\nOpposite Words (Pretrained Model):\nlg3: -0.14551663398742676\nEKPga: -0.14759600162506104\n9599: -0.14850187301635742\nENEbi: -0.14874517917633057\nkma: -0.15079879760742188\nHPN: -0.15138506889343262\n-õde: -0.1517009735107422\nPwc: -0.1537930965423584\nÕKE: -0.1545194387435913\na.2: -0.15509653091430664\n\n\nSimilar Words (Model):\nfountain: 0.8203656077384949\ncaptain: 0.6859565377235413\ncontain: 0.6147876977920532\nmaintain: 0.6075064539909363\nplantain: 0.5841538310050964\ncurtain: 0.536421000957489\ncertain: 0.4993521571159363\npain: 0.45486366748809814\nmaintainence: 0.4481288492679596\nmount: 0.4457419812679291\n\n\nOpposite Words (Model):\nemployer: 0.3933359980583191\nblech: 0.34710055589675903\nemployee: 0.3409126102924347\nfamous: 0.3316144645214081\nflavorful: 0.31955912709236145\nbo: 0.3150962293148041\nhalo: 0.3106391727924347\ngarlic: 0.3064032196998596\nmarvelous: 0.3034113943576813\nnail: 0.30082669854164124\n\n\n-------------------------------------------------------------------\nWord: notebook\n\nSimilar Words (Pretrained Model):\n0.8318610787391663: macbook\n0.8181561231613159: Notebook\n0.8092491030693054: Zenbook\n0.8016971349716187: e-book\n0.7996373772621155: Lifebook\n0.7921969890594482: ebook\n0.7896876931190491: smartbook\n0.784850001335144: workbook\n0.7829756140708923: Bluebook\n0.7755618691444397: .oo-ui-icon-ref-cite-book\n\n\nOpposite Words (Pretrained Model):\npHst: -0.04219520092010498\nUPP: -0.05822253227233887\nkSv: -0.05889594554901123\nOF2: -0.0602877140045166\nC-O: -0.06576263904571533\nBPH: -0.06969892978668213\ni16: -0.07449150085449219\nHIF: -0.07605993747711182\nCôa: -0.07811081409454346\ntüvve: -0.08198833465576172\n\n\nSimilar Words (Model):\nebook: 0.8247413039207458\nprebook: 0.6851521730422974\nbook: 0.6794581413269043\nfacebook: 0.6776096224784851\ntextbook: 0.6319820284843445\ncookbook: 0.563440203666687\nnote: 0.509781539440155\nnook: 0.4714672863483429\nsook: 0.438133180141449\nlook: 0.4227614998817444\n\n\nOpposite Words (Model):\nahhhhhhlike: 0.38684120774269104\nahhhhhhhhhhhhhh: 0.3633235692977905\nahhhhhh: 0.3547278642654419\nahhhhh: 0.34917348623275757\nahhhh: 0.32478705048561096\ncrawfish: 0.3100922107696533\npub: 0.30954569578170776\nstellar: 0.3077695071697235\ncriwde: 0.30374675989151\ncrowdedlast: 0.29789894819259644\n\n\n-------------------------------------------------------------------\nWord: orange\n\nSimilar Words (Pretrained Model):\n0.7588224411010742: Lorange\n0.7353426218032837: -range\n0.7277595400810242: orangesom\n0.722787618637085: midrange\n0.7202275395393372: Midrange\n0.7096782326698303: Esrange\n0.7087451815605164: D-range\n0.6948964595794678: Herserange\n0.6930824518203735: long-range\n0.6915110349655151: Fénétrange\n\n\nOpposite Words (Pretrained Model):\nENEbi: -0.05771327018737793\nLOVL: -0.06357359886169434\n.Aeg: -0.08549010753631592\nTLV: -0.10572445392608643\n8198: -0.13110888004302979\nFIEt: -0.13494503498077393\nONG: -0.13543915748596191\nTTÜM: -0.13814008235931396\nAV11: -0.13946294784545898\n.EL: -0.14279913902282715\n\n\nSimilar Words (Model):\nlorange: 0.9183123707771301\nrange: 0.7840638756752014\nfreerange: 0.682730495929718\nvanillaorange: 0.6089365482330322\norangecello: 0.5556274652481079\nstrange: 0.4975895583629608\nranger: 0.4391994774341583\nexchange: 0.3931232690811157\nclosely: 0.35912758111953735\nangelo: 0.3352895975112915\n\n\nOpposite Words (Model):\neducation: 0.34469109773635864\npopulation: 0.32720473408699036\nnation: 0.32114866375923157\nupdatable: 0.3007286489009857\nlocationand: 0.30066242814064026\ndonation: 0.2893359065055847\ncombination: 0.2892332077026367\ncommunication: 0.288646399974823\nlibation: 0.2854886054992676\nvalidation: 0.2846067249774933\n\n\n-------------------------------------------------------------------\nWord: penguin\n\nSimilar Words (Pretrained Model):\n0.7982094883918762: PenguinTV\n0.7734056711196899: Benguine\n0.7710337042808533: Penguin\n0.7657243609428406: Penguini\n0.6394851207733154: Penguins\n0.6334365010261536: loosPenguins\n0.6315305829048157: Béguin\n0.6301063895225525: Benguigui\n0.624904990196228: Viking-Penguin\n0.6182914972305298: Camiguin\n\n\nOpposite Words (Pretrained Model):\nM.T: -0.008194565773010254\n.b: -0.016735196113586426\nJJG: -0.02422618865966797\nEVC: -0.024962544441223145\n54-: -0.028854846954345703\nTIIM: -0.030907154083251953\nASBA: -0.03346896171569824\n2C-D: -0.03624296188354492\nLVN: -0.03641211986541748\nGä: -0.036983489990234375\n\n\nSimilar Words (Model):\npenns: 0.5977649092674255\npenne: 0.567247748374939\nlengua: 0.4758851230144501\nruin: 0.47012078762054443\npencil: 0.43829068541526794\nlens: 0.4247395098209381\npennsylvania: 0.42312073707580566\ntcj: 0.41830354928970337\njoels: 0.4124401807785034\npenang: 0.4070475399494171\n\n\nOpposite Words (Model):\ndiggity: 0.30555030703544617\nquote: 0.3025500774383545\ncozy: 0.30129384994506836\nsorbet: 0.2969212234020233\nvariety: 0.2966340482234955\ncheese: 0.28978976607322693\nbiggie: 0.28588560223579407\ncrossroad: 0.28464552760124207\nemployer: 0.27021047472953796\nopt: 0.2701180875301361\n\n\n-------------------------------------------------------------------\nWord: queen\n\nSimilar Words (Pretrained Model):\n0.6568859219551086: drag\n0.6478614807128906: Lueen\n0.5521672964096069: cock\n0.5507765412330627: queens\n0.5342242121696472: fucks\n0.5335116982460022: Snowqueen\n0.5281566381454468: cocks\n0.5224360823631287: sucks\n0.5212841629981995: hairy\n0.5189465284347534: naked\n\n\nOpposite Words (Pretrained Model):\nDÜNAMO: -0.20515108108520508\nEÜRP: -0.2112565040588379\nEEKK: -0.22450995445251465\n459.: -0.22932004928588867\nRJM-s: -0.23008394241333008\nFAI-s: -0.23246121406555176\nEOKi: -0.23668146133422852\nSDT: -0.23732173442840576\nKK-s: -0.24047768115997314\nPESi: -0.24182558059692383\n\n\nSimilar Words (Model):\neileen: 0.5965366363525391\nween: 0.5401182174682617\nagreen: 0.4800267517566681\nqueso: 0.4697580337524414\nque: 0.45724424719810486\ngreen: 0.4505281448364258\nowen: 0.42695164680480957\ngwen: 0.4171654284000397\ninbetween: 0.39888814091682434\nsheen: 0.3916938006877899\n\n\nOpposite Words (Model):\naccept: 0.3961278200149536\nrecover: 0.3540710508823395\nmold: 0.33477693796157837\nreceipt: 0.3180425763130188\nmohame: 0.31165897846221924\ntracker: 0.3097807765007019\ndds: 0.3077697455883026\nrest: 0.30438101291656494\npriceduntil: 0.301664263010025\ngrandkid: 0.3000657260417938\n\n\n-------------------------------------------------------------------\nWord: rabbit\n\nSimilar Words (Pretrained Model):\n0.8171772360801697: rabbits\n0.6216549873352051: rabbi\n0.6024944186210632: flies\n0.6015673279762268: walks\n0.5782260298728943: grows\n0.5673815011978149: gills\n0.5661486983299255: forebrain\n0.5652034878730774: Rabbits\n0.5636166930198669: muscles\n0.5634969472885132: lies\n\n\nOpposite Words (Pretrained Model):\nningb: -0.17822480201721191\nEOLi: -0.23430144786834717\n14st: -0.23593533039093018\nHKHK: -0.24051153659820557\nEKHHL: -0.24452710151672363\nMMide: -0.253261923789978\nEVLi: -0.25596463680267334\nEKAKis: -0.2561606168746948\nfüülis: -0.26305830478668213\nEMLi: -0.263637900352478\n\n\nSimilar Words (Model):\nduckrabbit: 0.749933660030365\nhobbit: 0.6577000617980957\nsauteed: 0.49623891711235046\nrawk: 0.4252173602581024\nrabe: 0.408352255821228\nsaut: 0.406775563955307\nbit: 0.37062859535217285\nqa: 0.36210256814956665\nbenoit: 0.3603163957595825\nraw: 0.35941821336746216\n\n\nOpposite Words (Model):\nmural: 0.39271992444992065\nlane: 0.3432596027851105\nprofessional: 0.33401796221733093\nunprofessional: 0.330081045627594\nplan: 0.32936355471611023\nplane: 0.3284943401813507\ngas: 0.3234637975692749\nupbeat: 0.31626200675964355\nlazy: 0.31122422218322754\ncollard: 0.30835363268852234\n\n\n-------------------------------------------------------------------\nWord: sunshine\n\nSimilar Words (Pretrained Model):\n0.8112868666648865: shine\n0.776638388633728: Whine\n0.7734240293502808: Sunshine\n0.772926390171051: Moonshine\n0.7544946074485779: thine\n0.7290497422218323: Thine\n0.727364182472229: Téchine\n0.7262041568756104: makuahine\n0.7251249551773071: chine\n0.719876766204834: yhine\n\n\nOpposite Words (Pretrained Model):\nKMSi: -0.1460132598876953\n7916: -0.15434229373931885\n8198: -0.16281485557556152\nEKPd: -0.16911542415618896\nNTAK: -0.1709803342819214\nÜFi: -0.17464816570281982\nloa-: -0.17543303966522217\nlg3: -0.18224263191223145\n.EL: -0.18310952186584473\nPKZ: -0.18371570110321045\n\n\nSimilar Words (Model):\nsunshiney: 0.8703379034996033\nshine: 0.715082585811615\nshin: 0.5639261603355408\nshiny: 0.540286660194397\nsunset: 0.5287966728210449\nsun: 0.4818324148654938\nshiz: 0.45051342248916626\nsunfun: 0.40978750586509705\nscary: 0.40171894431114197\nstephine: 0.3934682607650757\n\n\nOpposite Words (Model):\naladdin: 0.35407453775405884\nbudino: 0.32696905732154846\nbuxks: 0.3143126964569092\nancho: 0.31369492411613464\nantipasto: 0.2827610373497009\nbru: 0.28028902411460876\nbodino: 0.2769157290458679\nbull: 0.2737075686454773\nboat: 0.27121520042419434\ngag: 0.2699294090270996\n\n\n-------------------------------------------------------------------\nWord: tree\n\nSimilar Words (Pretrained Model):\n0.7195155620574951: B-tree\n0.7041049003601074: OCEANA\n0.6645926833152771: Bigtree\n0.6505767703056335: trees\n0.6149125695228577: OCEAN\n0.5975631475448608: potree\n0.5934450030326843: Tiptree\n0.586904764175415: OCEANDIVA\n0.5755794644355774: treegrow\n0.5741511583328247: Boxtree\n\n\nOpposite Words (Pretrained Model):\n241ss: -0.17451906204223633\nСДЛК: -0.18938016891479492\nEÜRP: -0.19196879863739014\nBenák: -0.1976855993270874\nНСЗРиС: -0.19882965087890625\nNKPga: -0.20064902305603027\nBugrõ: -0.20231139659881592\nM-19: -0.20331108570098877\nВРК: -0.20352685451507568\nсов: -0.2058929204940796\n\n\nSimilar Words (Model):\ntreenut: 0.6992639899253845\ntrek: 0.6853687167167664\ngreentree: 0.6010702252388\ntre: 0.5962489247322083\ntres: 0.5897501707077026\nbree: 0.5146178007125854\nentree: 0.5119525194168091\ndegree: 0.49524039030075073\ntrend: 0.4815516173839569\ntrentaaaaa: 0.47382205724716187\n\n\nOpposite Words (Model):\ncompress: 0.3741748631000519\nexpress: 0.32312214374542236\ncomputer: 0.3188527822494507\ncypress: 0.3055906593799591\njordan: 0.2886573374271393\nhiv: 0.28798940777778625\njambalaya: 0.2836908996105194\nbuttergarlic: 0.2795211970806122\nrib: 0.2765409052371979\nvary: 0.2736755609512329\n\n\n-------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 10. Extract CSV Table for Results","metadata":{}},{"cell_type":"code","source":"import csv\n\ndef export_to_csv(pretrained_results, scratch_results, output_file):\n    with open(output_file, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n\n        # Write header row\n        writer.writerow([\"Word\", \"Pretrained Similar Word 1\", \"Pretrained Similarity 1\",\n                         \"Pretrained Opposite Word 1\", \"Pretrained Opposite Similarity 1\",\n                         \"Scratch Similar Word 1\", \"Scratch Similarity 1\",\n                         \"Scratch Opposite Word 1\", \"Scratch Opposite Similarity 1\"])\n\n        # Write data rows\n        for word in pretrained_results['similar_words']:\n            pretrained_similar = pretrained_results['similar_words'][word]\n            pretrained_opposite = pretrained_results['opposite_words'][word]\n            scratch_similar =  scratch_results['similar_words'][word]\n            \n            scratch_opposite = scratch_results['opposite_words'][word]\n            \n            row = [word]\n\n            for i in range(10):\n                if i < len(pretrained_similar):\n                    row.extend(pretrained_similar[i])\n                else:\n                    row.extend([\"\", \"\"])\n\n                if i < len(pretrained_opposite):\n                    row.extend(pretrained_opposite[i])\n                else:\n                    row.extend([\"\", \"\"])\n\n                if i < len(scratch_similar):\n                    row.extend(scratch_similar[i])\n                else:\n                    row.extend([\"\", \"\"])\n\n                if i < len(scratch_opposite):\n                    row.extend(scratch_opposite[i])\n                else:\n                    row.extend([\"\", \"\"])\n\n                writer.writerow(row)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:26:37.930899Z","iopub.execute_input":"2024-04-20T14:26:37.931595Z","iopub.status.idle":"2024-04-20T14:26:37.941893Z","shell.execute_reply.started":"2024-04-20T14:26:37.931563Z","shell.execute_reply":"2024-04-20T14:26:37.940926Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"export_to_csv(pretrained_results, scratch_results, \"word_embedding_comparison22.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:26:38.642915Z","iopub.execute_input":"2024-04-20T14:26:38.643661Z","iopub.status.idle":"2024-04-20T14:26:38.659439Z","shell.execute_reply.started":"2024-04-20T14:26:38.643622Z","shell.execute_reply":"2024-04-20T14:26:38.658455Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"model.save('fast-text-v0.bin')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T12:31:24.984732Z","iopub.execute_input":"2024-04-20T12:31:24.985164Z","iopub.status.idle":"2024-04-20T12:31:25.598776Z","shell.execute_reply.started":"2024-04-20T12:31:24.985132Z","shell.execute_reply":"2024-04-20T12:31:25.597841Z"},"trusted":true},"execution_count":59,"outputs":[]}]}